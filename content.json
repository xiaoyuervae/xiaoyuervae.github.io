[{"title":"分布式锁的应用","date":"2019-01-22T15:17:09.000Z","path":"posts/undefined/","text":"要解决什么问题？现在在互联网的场景中大家的服务应该都是进行分布式部署的，分布式锁和我们之前Java应用中的锁其实都是为了解决多线程的问题。只是说在以前单机部署情况下使用的如 Synchronized、Lock 等锁在分布式的场景下某些业务无法满足，需要我们使用分布式锁来解决这一分布式竞争的情况。 分布式锁会出现哪样的一些问题？互斥（只有一个客户端能获取锁）互斥是最基本的、在同一时刻只能有一个客户端能够获取到锁，这个通过数据库、Redis、Zookeeper等特性都能够保证。 不能死锁死锁、顾名思义，就是说锁一直都不被释放，导致其他线程或任务一直都阻塞在这里，永远都不能获取到锁（如果是阻塞锁可能会使任务一直堆积）。 不能死锁很多人也会回答说给我们的锁来设置一个过期时间（比如说Redis锁） 这边给一个我们线上出现死锁的情况，我们来看一下这段代码: 123456789101112public boolean tryLock(String key, int timeToLive) throws Exception &#123; if (timeToLive &lt;= 0) &#123; timeToLive = TTL; &#125; Long v = jedisCluster.incr(key); if (target.equals(v)) &#123; jedisCluster.expire(key, timeToLive); return true; &#125; return false;&#125; 我们看到这段代码中是使用JedisCluster.incr(key)来创建分布式锁的，如果说获取到锁了那么返回结果应该是1L，成功后便设置锁的过期时间，这段代码看起来是没问题，但是在我们线上的情况竟然发现了死锁！ 这是什么情况呢，某天发现该分布式锁下面代码逻辑都没有再执行，到redis里面查看了该key的值发现竟然有7w多，明显是执行了7w多次tryLock最终还是没有能够获取到锁，但是难道一次代码逻辑需要执行这么多次吗？很明显不是，按道理来讲如果锁了一定时间后这个key应该是会过期的，我们的timeToLive设置的值是60s，不会说一直在这里不释放的。 后面查看应用日志发现我们的redis在某1s内突然失去了连接（具体原因不明）, 于是恍然大悟, 在我们tryLock的逻辑中是先设置key再去设置过期时间，这两者并不是一个原子性的操作，可能刚好在这个时间点我们设置好了key的值但是没来得及设置过期时间 redis 失去了连接，导致我们的过期时间设置失败，所以这个key一直没能够释放。 明白了原因我们就知道这种分布式锁的实现方式其实是存在明显漏洞的，要保证给锁设置值和过期时间是一个原子性的操作才能够保证不被死锁。 我们把这个方式直接改成了使用redis命令来设置: SET my:lock 随机值 NX PX 30000 执行这个命令就 ok。 NX：表示只有 key 不存在的时候才会设置成功。（如果此时 redis 中存在这个 key，那么设置失败，返回 nil） PX 30000：意思是 30s 后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。 容错我这边理解的容错可能需要应对以下几种情况: 分布式锁依赖的中间件不可用(如Redis、Zookeeper、数据库等) 应用突然宕机（包括应用发布过程） 中间件集群（如Redis集群中只要大部分节点创建了这把锁就可以，N个Redis节点，需要 (N/2 + 1) 个节点以上） 过期时间超时后不能让该线程解锁 我这边讲一下最后一个点，也就是说如果分布式锁过期了但是该线程中的任务还没有结束，当该线程的执行逻辑结束之后其实之前他自己获得的锁已经被释放了，这时候它如果再去做解锁的操作其实是将其他线程获取的锁给释放掉了，这种情况肯定是会有问题的。 所以在分布式锁释放的时候我们必须保证一个线程释放的锁必须是这个线程获得的，针对Redis分布式锁来说我们可以使用lua脚本来做释放锁的操作: 123456-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1])else return 0end 这里我们需要判断value，这个value是我们在tryLock的时候去创建的，我们可以使用UUID之类的随机数（要确保每次创建的都不会一致）来创建、然后在删除的时候去判断这个value，不相同说明不是该线程创建的。 实现分布式锁的几种方式其实实现分布式锁的方式有很多种，基于Redis的我上面基本都介绍了，我这边着重讲一下基于Zookeeper来实现分布式锁。 zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class ZooKeeperSession &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; private CountDownLatch latch; public ZooKeeperSession() &#123; try &#123; this.zookeeper = new ZooKeeper(\"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\", 50000, new ZooKeeperWatcher()); try &#123; connectedSemaphore.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"ZooKeeper session established......\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取分布式锁 * * @param productId */ public Boolean acquireDistributedLock(Long productId) &#123; String path = \"/product-lock-\" + productId; try &#123; zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; &#125; catch (Exception e) &#123; while (true) &#123; try &#123; // 相当于是给node注册一个监听器，去看看这个监听器是否存在 Stat stat = zk.exists(path, true); if (stat != null) &#123; this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; &#125; catch (Exception ee) &#123; continue; &#125; &#125; &#125; return true; &#125; /** * 释放掉一个分布式锁 * * @param productId */ public void releaseDistributedLock(Long productId) &#123; String path = \"/product-lock-\" + productId; try &#123; zookeeper.delete(path, -1); System.out.println(\"release the lock for product[id=\" + productId + \"]......\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 建立zk session的watcher * */ private class ZooKeeperWatcher implements Watcher &#123; public void process(WatchedEvent event) &#123; System.out.println(\"Receive watched event: \" + event.getState()); if (KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; if (this.latch != null) &#123; this.latch.countDown(); &#125; &#125; &#125; /** * 封装单例的静态内部类 * */ private static class Singleton &#123; private static ZooKeeperSession instance; static &#123; instance = new ZooKeeperSession(); &#125; public static ZooKeeperSession getInstance() &#123; return instance; &#125; &#125; /** * 获取单例 * * @return */ public static ZooKeeperSession getInstance() &#123; return Singleton.getInstance(); &#125; /** * 初始化单例的便捷方法 */ public static void init() &#123; getInstance(); &#125;&#125; 也可以采用另一种方式，创建临时顺序节点： 如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁；后面的每个人都会去监听排在自己前面的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 zookeeper 给通知，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ZooKeeperDistributedLock implements Watcher &#123; private ZooKeeper zk; private String locksRoot = \"/locks\"; private String productId; private String waitNode; private String lockNode; private CountDownLatch latch; private CountDownLatch connectedLatch = new CountDownLatch(1); private int sessionTimeout = 30000; public ZooKeeperDistributedLock(String productId) &#123; this.productId = productId; try &#123; String address = \"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\"; zk = new ZooKeeper(address, sessionTimeout, this); connectedLatch.await(); &#125; catch (IOException e) &#123; throw new LockException(e); &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; &#125; public void process(WatchedEvent event) &#123; if (event.getState() == KeeperState.SyncConnected) &#123; connectedLatch.countDown(); return; &#125; if (this.latch != null) &#123; this.latch.countDown(); &#125; &#125; public void acquireDistributedLock() &#123; try &#123; if (this.tryLock()) &#123; return; &#125; else &#123; waitForLock(waitNode, sessionTimeout); &#125; &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; &#125; public boolean tryLock() &#123; try &#123; // 传入进去的locksRoot + “/” + productId // 假设productId代表了一个商品id，比如说1 // locksRoot = locks // /locks/10000000000，/locks/10000000001，/locks/10000000002 lockNode = zk.create(locksRoot + \"/\" + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 看看刚创建的节点是不是最小的节点 // locks：10000000000，10000000001，10000000002 List&lt;String&gt; locks = zk.getChildren(locksRoot, false); Collections.sort(locks); if(lockNode.equals(locksRoot+\"/\"+ locks.get(0)))&#123; //如果是最小的节点,则表示取得锁 return true; &#125; //如果不是最小的节点，找到比自己小1的节点 int previousLockIndex = -1; for(int i = 0; i &lt; locks.size(); i++) &#123; if(lockNode.equals(locksRoot + “/” + locks.get(i))) &#123; previousLockIndex = i - 1; break; &#125; &#125; this.waitNode = locks.get(previousLockIndex); &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; return false; &#125; private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(locksRoot + \"/\" + waitNode, true); if (stat != null) &#123; this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; public void unlock() &#123; try &#123; // 删除/locks/10000000000节点 // 删除/locks/10000000001节点 System.out.println(\"unlock \" + lockNode); zk.delete(lockNode, -1); lockNode = null; zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; public class LockException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public LockException(String e) &#123; super(e); &#125; public LockException(Exception e) &#123; super(e); &#125; &#125;&#125; 基于Redis 和 Zk 来实现分布式锁其实有一个最本质的区别: redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。 另外一点就是，如果是 redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。 但是，在我们高并发的场景下，基于Zookeeper实现的分布式锁的qps不能太高，其性能是要低于Redis的，所以总的来说，基于Zookeeper和 Redis 实现的分布式锁各有优缺点，大家在选择的时候可以根据业务具体去看哪一种符合你的业务场景。","tags":[]},{"title":"RocketMq 学习之核心参数","date":"2018-12-10T22:53:23.000Z","path":"posts/1044148734/","text":"producerGroup 生产者组名，一个服务默认只允许配置唯一一个生产组名。 createTopicKey 自动创建topic key。生产环境不建议使用，尽量是由公司的架构组提供统一对RocketMq的API进行二次封装，隐藏一些API。 defaultTopicQueueNums 一个topic下默认的消息队列，默认是4个 compressMsgBodyHowmuch 消息体自动压缩、默认为 4096 个字节、便于在网络中进行传输。 retryTimesWhenSendFailed 消息发送失败后重试的次数、可配置，这是针对于同步发送消息的，异步发送消息对应有另外的配置 retryTimesWhenSendAsyncFailed。 retryAnotherBrokerWhenNotStoreOK 如果在broker上存储失败可以对其他broker进行重试，默认为false maxMessageSize 最大消息大小，默认为128k producer的配置项:","tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"https://blog.xiaoyuervae.cn/tags/rocketmq/"}]},{"title":"一次排查MongoDB CPU过高、服务无响应的问题","date":"2018-12-02T14:59:38.000Z","path":"posts/611001842/","text":"问题的出现IM内的聊天记录等数据都存在Mongo DB中、最近手机短信一直收到持续不断的Apdex值比较低的告警，所以集中花时间看了一下到底是什么问题造成的。 IMG_3045 在我们的Marvin监控系统中看到出现的慢日志和相关错误日志中看到基本上都来自于同一个接口: 查看具体代码日志发现都是在执行查询Mmongo的时候造成的超时。 排查具体的问题现象在观察这个接口一段时间之后发现一个非常奇特的现象: 该接口正常情况下的rt还是很正常的, 然后突然rpm值会激增,同时rt也会激增, 从正常情况下的 5ms 左右突然激增到 2s 的时间左右, 这对于一个正常的接口来说是不可接受的。 观察了一下mongo机器的 zabbix 监控, 查看了一下mongo服务器在当时的状态, 发现除了机器的 cpu load 有稍微的波动之外, 磁盘IO、内存等数值都没有明显的上涨: 观察了线上mongo本身的日志, 发现在rt上涨的过程中 mongo 服务器的连接数不断在上涨, 从平稳的连接数95开始上涨, 1s钟之内上涨到了1500左右的连接,同时在这段期间貌似是没有任何返回,从日志中可以看到一个连接的响应时间的开始到结束有些甚至达到了80s之久！(什么sql能查这么久?!) 排查联系了运维的同学帮忙看一下到底是什么问题造成的, 因为运维同学最近比较忙, 恰巧碰到他负责双十二全链路压测, 而且他主要也是运维MySQL的经验比较多, 帮我们粗略看了一下认为是并发太高的问题造成的(突然一时间并发连接数暴涨)。一时之间也看不出来有什么大的问题。 和运维同学一起排查的过程中我猜想是不是由于部分SQL没有走索引, 导致全表扫描或者是查询效率特别慢造成的阻塞, 运维同学回答我说有可能, 为了验证这个想法我们查看了在那段时间内的服务端日志中打印出来的慢查询的sql, 发现sql实在太多, 随便拉了几条出来看: 123command historymessage.$cmd command: count &#123; count: \"of_history_message\", query: &#123; session: \"86690631_97553434\", $and: [ &#123; msgTime: &#123; $lte: 1543226394744 &#125; &#125; ] &#125; &#125; planSummary: COUNT_SCAN &#123; session: 1.0, msgTime: -1.0 &#125; keyUpdates:0 writeConflicts:0 numYields:0 reslen:44 locks:&#123; Global: &#123; acquireCount: &#123; r: 1 &#125; &#125;, MMAPV1Journal: &#123; acquireCount: &#123; r: 1 &#125; &#125;, Database: &#123; acquireCount: &#123; r: 1 &#125; &#125;, Collection: &#123; acquireCount: &#123; R: 1 &#125; &#125; &#125; 71270ms 123&#123; count: \"of_history_message\", query: &#123; session: \"86492947_95561724\", $and: [ &#123; msgTime: &#123; $lte: 1543235835000 &#125; &#125; ] &#125; &#125; planSummary: COUNT_SCAN &#123; session: 1.0, msgTime: -1.0 &#125; keyUpdates:0 writeConflicts:0 numYields:0 reslen:44 locks:&#123; Global: &#123; acquireCount: &#123; r: 1 &#125; &#125;, MMAPV1Journal: &#123; acquireCount: &#123; r: 1 &#125; &#125;, Database: &#123; acquireCount: &#123; r: 1 &#125; &#125;, Collection: &#123; acquireCount: &#123; R: 1 &#125; &#125; &#125; 151ms 看了一下这些sql 并在mongo 上打印了一下执行计划, 都是很正常的sql, 并且都走了索引, 同时放到mongo 上执行速度也很快，这些sql应该都是因为前面的执行sql发生了阻塞之后造成响应慢的。 为了能够更好的进行排查我们从运维的同学那边要到了mongo服务器的机器权限, 方便进行更好的排查, 势必是要解决当前的这个问题。拿到服务器的权限之后发现mongo 的服务器配置还是相当强悍的： 3台物理机, 逻辑CPU有48个, 内存有250G! CPU信息 内存信息 线上以Mongo副本集的方式部署, 这样的配置可谓是相当强悍, 所有mongo数据库中数据基本上都是加载到内存, 查询起来按理来说不会很慢的啊。 发现问题线上mongo默认的日志好像是好几个月的日志都打在了同一个文件中，日志量实在太大，连grep一下都很难，只能看近期的一些日志，所有要做排查实在是非常困难。 既然在mongo服务器上看不出问题那就从客户端出发吧，既然是在某一个时间点突然爆发出现速度慢的情况我是不是只要找出那个第一条响应慢的日志就可以知道到底是什么问题了。 于是我在客户端查询的接口上加上了具体的调用日志，打印出每条查询mongo的sql的开始时间和结束时间，并且把对应的sql打印出来，上线之后观察一段时间，我找到了那第一条执行速度慢的日志: 我发现这条查询语句非常奇怪，竟然是查询小于当前时间的数量, 奇怪归奇怪, 但是讲道理也不应该会那么慢的, 只是查询一个数量而已, 于是我把这条语句放到了mongo上查看了一下执行计划: 不看不知道, 执行了一下发现竟然没有返回,并且出现了报错, 于是我立马意识到这条sql可能压根都没有索引, 查询小于当前时间所有的数据进行了一次全表扫描: 8000多万数据进行了一次无索引的count, 查看了一下collection上的索引: 发现果然是没有这个索引！ 解决问题发现了这个慢sql之后我非常好奇为什么会有这个sql出现, 同客户端的同学确认了一下之后发现这个sql的存在很有可能是由于参数的误传导致的, 因为这个接口中mongo查询的参数是由前端的同学传过来，sql经过参数的拼接而形成的。 为了验证我的想法我在业务的代码中增加了对这种sql形成的过滤(当然可能不止存在这一种没有走索引的慢sql)。 代码上线之后得到了惊人的效果, 慢sql的情况居然一去不复返了。接口的rt再也没有上过30ms, 说明暂时还没有其他的慢sql出现: 同时接口的rpm值也随之降下来了, 这时我意识到rpm的上涨应该是由于客户端超时之后进行了重试, 导致rpm值一下子暴涨, 所以mongo服务器的连接数才会一下子暴涨。 至此，问题终于得到了解决，这几天终于没有再收到告警，舒坦了。。。 总结这次mongo服务器问题的排查经历了大概三天左右的时间才最终把问题找到解决, 也是我经历的最久的一次问题排查了，整个过程是比较艰辛的，虽然过程是很简单，但是中间是绕了不少的弯路，也查阅了不少mongo相关的资料，发现mongo这块的资料真的是少，不过好在最后找到了问题所在。以前排查问题很少有这样的坚持力，基本上排查了一段时间后如果说真的发现不了问题的话就放弃了，所以对待这种排查起来比较困难的问题还是要懂得坚持，理清自己的思路。 通过这次排查我也发现我们之前在使用mongo的时候也有很多不规范的地方，mongo是一个性能很强悍的NoSQL数据库, 我们可能还没有真正的发挥出Mongo自身的性能。同时也有几个不规范的点: 在业务中拼条件组成sql，可能大家为了代码的复用方便不愿去写多个SQL分别去做查询，但是这样造成了有些参数的缺失可能形成了一条性能极差的sql。 客户端的超时次数没有设置好，这次的问题也主要是由于客户端的超时次数太多了，其实一两条这种sql根本并不会造成严重的性能问题的，主要是因为超时重试造成了同一时间内多条这种低性能SQL阻塞了mongo的执行甚至拖垮整个mongo集群。 mongo服务的监控做的不够到位，这个问题也不好说，mongo在公司的使用毕竟还不够主流，相关工具的支持也很不完善，出现了问题没有能够进行及时的反馈，导致这个问题在线上出现了很久。","tags":[{"name":"mongo","slug":"mongo","permalink":"https://blog.xiaoyuervae.cn/tags/mongo/"},{"name":"cpu","slug":"cpu","permalink":"https://blog.xiaoyuervae.cn/tags/cpu/"}]},{"title":"记一次排查线上CPU飚高的问题","date":"2018-11-22T22:08:23.000Z","path":"posts/1407851176/","text":"记一次排查线上机器CPU飙升的问题最近我们的报表系统经常受到短信和邮件告警。 由于这个服务是专门用来做报表导出的、每天在跑各种报表任务、做XML解析，很多情况下还是多个任务在跑的，所以之前也没怎么关心，运维也反馈过告警有点频繁。 安排组里相关同学看了一下，也做了一些导出的限制，但是效果好像不是很好。 昨天早上系统疯狂告警、从8点钟开始告警，然后看了一下相关的问题。下面是具体的排查过程 查看机器相关状态在监控系统中查看了一下机器的CPU和内存的状态: 机器整天的CPU占用状态达到了90%、初步判断有相关线程在进行相关耗资源的操作（可能是死循环、频繁IO等） 于是打算到机器上查看当前占用CPU资源的线程。 使用Arthas的thread命令查看线程占用资源状态arthas是Alibaba开源的Java诊断利器、应该是今年上半年开源的，使用起来非常方便。这里贴出来github地址: Arthas 使用thread 命令查看当前线程信息、查看线程的堆栈。 我装好arthas之后使用thread命令查看了当前最忙的5个线程并打印堆栈, 发现占用CPU资源最高的是这个线程pool-9-thread-1(开发竟然没对线程池进行命名orz…): 这个线程竟然整整占了一个核心的资源（机器配置4C 4G） 同时我们能在打印的堆栈中看到如下信息 12345- locked java.io.FileNotFoundException---Number of locked synchronizers = 1 说明当前线程阻塞住了其他线程、应用卡主了，通常是由于线程拿住了某个锁， 并且其他线程都在等待这把锁造成的。 定位相关代码、分析问题原因 找到问题出现的代码在这里： 1doc = DocumentHelper.parseText(message.getXml) 先不要吐槽代码写的好不好，当时看到这个就有点懵逼了，这只是一句正常的XML解析的逻辑，用了dom4j里面的解析文本的方法，怎么就能够把CPU跑满呢？ 带着问题我看了一下里面的具体实现, 里面大体的逻辑是先去获取XMLReader, 即选取XML解析器，然后对XML进行解析。我们看到线程打印的堆栈最终是出现了FileNotFoundException, 那为什么会出现这个异常呢? JDK设计模式 - Service Provider机制（SPI） 为什么要说这个呢，因为在上面的选取XML解析器的过程中就用到了这个机制，抱着好奇的心态我debug了一下这个parseText代码, 在选取 XML 解析器的代码中: 这个FactoryFinder.find()是寻找XML解析器的逻辑: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061static Object find(String factoryId, String fallbackClassName) throws ConfigurationError &#123; dPrint(\"find factoryId =\" + factoryId); // Use the system property first try &#123; String systemProp = ss.getSystemProperty(factoryId); if (systemProp != null) &#123; dPrint(\"found system property, value=\" + systemProp); return newInstance(systemProp, null, true); &#125; &#125; catch (SecurityException se) &#123; if (debug) se.printStackTrace(); &#125; // try to read from $java.home/lib/jaxp.properties try &#123; String factoryClassName = null; if (firstTime) &#123; synchronized (cacheProps) &#123; if (firstTime) &#123; String configFile = ss.getSystemProperty(\"java.home\") + File.separator+ \"lib\" + File.separator + \"jaxp.properties\"; File f = new File(configFile); firstTime = false; if (ss.doesFileExist(f)) &#123; dPrint(\"Read properties file \"+f); cacheProps.load(ss.getFileInputStream(f)); &#125; &#125; &#125; &#125; factoryClassName = cacheProps.getProperty(factoryId); if (factoryClassName != null) &#123; dPrint(\"found in $java.home/jaxp.properties, value=\" + factoryClassName); return newInstance(factoryClassName, null, true); &#125; &#125; catch (Exception ex) &#123; if (debug) ex.printStackTrace(); &#125; // Try Jar Service Provider Mechanism Object provider = findJarServiceProvider(factoryId); if (provider != null) &#123; return provider; &#125; if (fallbackClassName == null) &#123; throw new ConfigurationError( \"Provider for \" + factoryId + \" cannot be found\", null); &#125; dPrint(\"loaded from fallback value: \" + fallbackClassName); return newInstance(fallbackClassName, null, true); &#125; 总共是下面四步: 第一步： 检查系统属性（systemProperty）是否设置了javax.xml.parsers.SAXParserFactory 属性值，如果设置了，就使用设置的属性值来返回SAXParserFactory 对象。 12345String systemProp = ss.getSystemProperty(factoryId); if (systemProp != null) &#123; dPrint(\"found system property, value=\" + systemProp); return newInstance(systemProp, null, true);&#125; 若没找到，进行第二步 第二步：检查java.home/lib/jaxp.properties 属性配置文件中是否有javax.xml.parsers.SAXParserFactory。 123456789101112131415161718// try to read from $java.home/lib/jaxp.propertiestry &#123; String factoryClassName = null; if (firstTime) &#123; synchronized (cacheProps) &#123; if (firstTime) &#123; String configFile = ss.getSystemProperty(\"java.home\") + File.separator + \"lib\" + File.separator + \"jaxp.properties\"; File f = new File(configFile); firstTime = false; if (ss.doesFileExist(f)) &#123; dPrint(\"Read properties file \"+f); cacheProps.load(ss.getFileInputStream(f)); &#125; &#125; &#125; &#125; factoryClassName = cacheProps.getProperty(factoryId); 有则返回，无则进入第三步 第三步： 使用jar包的Service Provider机制，加载工厂类。也就是说，查找所有加载的jar包中META-INF/services目录下的配置文件，文件名为 javax.xml.parsers.SAXParserFactory。文件的内容就是该jar包内提供的类实例。比如xercesImple-2.x.x.jar提供的org.apache.xerces.jaxp.SAXParserFactoryImpl. 12345// Try Jar Service Provider MechanismObject provider = findJarServiceProvider(factoryId);if (provider != null) &#123; return provider;&#125; 如果找不到，则进入最后第四步，使用默认的配置第四步：使用系统默认的工厂类： com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl。 1234567if (fallbackClassName == null) &#123; throw new ConfigurationError( \"Provider for \" + factoryId + \" cannot be found\", null);&#125;dPrint(\"loaded from fallback value: \" + fallbackClassName);return newInstance(fallbackClassName, null, true); 在没有进行任何配置以及使用第三方jar包的情况下，最终程序都会走到第四步，返回系统默认的工厂类。 得出结论 刚好我们的代码就是走到了这里面的逻辑, 寻找XML解析器里面的逻辑上面四步都走了，然后返回默认的工厂类, 且每次都会去通过SPI到加载的jar包中寻找文件, 找不到便会 lock 住FileNotFoundException, 执行一次这个代码可能还好, 但是这段导报表的逻辑是需要循环差不多9w次！！！ 9w次不断的遍历jar包寻找文件，然后不断的抛出 FileNotFoundException, 怪不得会把CPU飙升到90%。 其实我也在想为什么遍历完一次之后这边的解析逻辑中为什么不会把解析后的逻辑缓存一份，下次不用再去寻找这个文件，有点困惑、、、 提出解决办法、进行尝试找到问题之后问题就比较好解决了，主要是因为去使用SPI机制寻找jar文件的过程会比较消耗CPU，我们只要不让他去寻找，早先设置好默认的工厂类就好了。 可选择的方法应该有如下四种: 1.在运行java时，通过设置系统属性1java -D javax.xml.parsers.SAXParserFactory=new com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl 2.方法二:在程序中调用1System.setProperty(“javax.xml.parsers.SAXParserFactory”,” com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl”) 来设定实际的XML解析器. 3.方法三:编写一个jaxp.properties文件，在其中加入如下内容 1javax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl 再将此文件放入JAVA_HOME/lib/下 4.方法四:在打得jar包下，在目录META-INF/下新建一个services目录，在此目录新建一个文件名为 javax.xml.parsers.SAXParserFactory 的文件，文件内容写上实际使用的解析器类，如写 com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl ，这样在加载jar包之后，就会找到这个xml解析工厂。 最终我这边为了方便采取了第二种方式: 上线验证结果 上面是上线后第二天观察到的机器load情况，可以看到有很明显的改善！ 总结通过这次排查问题，我对JAXP有一个比较深的了解，其实，JAVA中有许多这种思想（Service Provider）的做法，这种思想，，就是平台无关性，发展到不依赖于具体的实现。 如我们熟悉的JNDI，JDBC，JAXP等。JNDI是抽像各种目录服务操作的类库，因为目录服务器厂商太多了，如SUN公司的ldapsdk, 还有novell公司等等。JDBC是抽像各种数据库操作的类库，因为数据库厂商也太多了，如ORACLE，SQLSERVER，MYSQL，INFORMIX等等。JAXP就是抽像各种XML解析器和转换器产品的类库，因为XML解析器和转换器产品够多的了。sun公司自己实现的HttpServer也是基于Service Provide机制。","tags":[{"name":"java","slug":"java","permalink":"https://blog.xiaoyuervae.cn/tags/java/"}]}]